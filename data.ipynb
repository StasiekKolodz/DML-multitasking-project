{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from skimage import color # For rgb2la & lab2rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = './data'\n",
    "BATCH_SIZE = 32\n",
    "IMG_SIZE = 224\n",
    "NUM_WORKERS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorizationDataset(Dataset):\n",
    "    def __init__(self, paths, split='train'):\n",
    "        if split == 'train':\n",
    "            self.transforms = transforms.Compose([\n",
    "                transforms.Resize((IMG_SIZE, IMG_SIZE),  Image.BICUBIC),\n",
    "                # (Train-only) data augmentation\n",
    "                # TODO: Try different variants? E.g.\n",
    "                # · random cropping (difficult to classify if subject is out of frame...)\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "            ])\n",
    "        elif split == 'val':\n",
    "            self.transforms = transforms.Resize((IMG_SIZE, IMG_SIZE),  Image.BICUBIC)\n",
    "\n",
    "        self.split = split\n",
    "        self.paths = paths\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.paths[idx]\n",
    "        img_rgb = Image.open(img_path).convert(\"RGB\")\n",
    "        img_rgb = self.transforms(img_rgb)\n",
    "        img_rgb = np.array(img_rgb)\n",
    "        img_lab = color.rgb2lab(img_rgb).astype(\"float32\")\n",
    "        img_lab = transforms.ToTensor()(img_lab)\n",
    "        # TODO: Understand this (why not use a Torch transform?)\n",
    "        L  = img_lab[[0], ...] / 50. - 1. # Between -1 and 1\n",
    "        ab = img_lab[[1, 2], ...] / 110. # Between -1 and 1\n",
    "        \n",
    "        # Store L and ab channels, as well as class\n",
    "        return { 'L': L, 'ab': ab, 'class': img_path.split('/')[-2] }\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 6500 samples · 204 batches\n",
      "Val: 250 samples · 8 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/damastah/miniconda3/envs/dml-gpu/lib/python3.9/site-packages/torchvision/transforms/transforms.py:280: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_paths = []\n",
    "val_paths = []\n",
    "\n",
    "for subdir, dirs, files in os.walk(DATA_DIR + \"/train\"):\n",
    "    if len(files) == 0: continue\n",
    "    for file in files:\n",
    "        if file.endswith('.JPEG'):\n",
    "            train_paths.append(subdir + \"/\" + file)\n",
    "\n",
    "for subdir, dirs, files in os.walk(DATA_DIR + \"/val\"):\n",
    "    if len(files) == 0: continue\n",
    "    for file in files:\n",
    "        if file.endswith('.JPEG'):\n",
    "            val_paths.append(subdir + \"/\" + file)\n",
    "\n",
    "train_dataset = ColorizationDataset(train_paths, split='train')\n",
    "val_dataset   = ColorizationDataset(val_paths,   split='val')\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, pin_memory=True, shuffle=True)\n",
    "val_dataloader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, pin_memory=True, shuffle=True)\n",
    "\n",
    "print(\"Train:\", len(train_dataset), \"samples ·\", len(train_dataloader), \"batches\")\n",
    "print(\"Val:\", len(val_dataset), \"samples ·\", len(val_dataloader), \"batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 224, 224]) torch.Size([32, 2, 224, 224]) ['lionfish', 'sea_anemone', 'sea_cucumber', 'sea_anemone', 'pufferfish', 'sea_cucumber', 'lionfish', 'sea_cucumber', 'sea_anemone', 'sea_anemone', 'pufferfish', 'sea_anemone', 'sea_snake', 'pufferfish', 'sea_snake', 'sea_snake', 'sea_cucumber', 'pufferfish', 'pufferfish', 'sea_anemone', 'sea_cucumber', 'sea_anemone', 'pufferfish', 'sea_snake', 'sea_snake', 'sea_cucumber', 'lionfish', 'sea_snake', 'pufferfish', 'pufferfish', 'lionfish', 'lionfish']\n"
     ]
    }
   ],
   "source": [
    "# Sanity check: looks good!\n",
    "for batch in train_dataloader:\n",
    "    print(batch['L'].shape, batch['ab'].shape, batch['class'])\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dml-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
